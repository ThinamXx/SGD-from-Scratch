# -*- coding: utf-8 -*-
"""SGD from Scratch

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17J42Qn-H1j7jcI2_cZEP-RCQqyq3ankv

### **Initialization**
"""

# !curl -s https://course.fast.ai/setup/colab | bash

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline

"""### **Downloading all Dependencies.**"""

from fastai.basics import *

"""**Linear Regression**"""

n = 200

x = torch.ones(n, 2)
x[:, 0].uniform_(-1., 1)
x[:5]

a = tensor(3., 2)
a

y = x@a + 0.25*torch.randn(n)

plt.scatter(x[:, 0], y)

def mse(y_hat, y):
  return ((y_hat - y)**2).mean()

a = tensor(-1., 1)

y_hat = x@a
mse(y_hat, y)

plt.scatter(x[:, 0], y)
plt.scatter(x[:, 0], y_hat)

"""### **Gradient Descent**"""

a = nn.Parameter(a)
a

def update():
  y_hat = x@a
  loss = mse(y, y_hat)
  if t%10 == 0:
    print(loss)
  loss.backward()
  with torch.no_grad():
    a.sub_(lr*a.grad)
    a.grad.zero_()

lr = 1e-1
for t in range(200):
  update()

plt.scatter(x[:, 0], y)
plt.scatter(x[:, 0], x@a.detach())

"""### **Animation**"""

from matplotlib import animation, rc
rc('animation', html='jshtml')

a = nn.Parameter(tensor(-1., 1))

fig = plt.figure()
plt.scatter(x[:, 0], y, c="orange")
line, = plt.plot(x[:, 0], x@a.detach())
plt.close()

def animate(i):
  update()
  line.set_ydata(x@a.detach())
  return line,

animation.FuncAnimation(fig, animate, np.arange(1, 200), interval=20)